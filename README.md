# Robustness Evaluation of Language Models to Controlled Misspellings

## Introduction

This study evaluates the robustness of various large language models (LLMs) to controlled syntactic perturbations — specifically, misspellings. The goal is to quantify how resilient different models are to increasing levels of input noise by measuring how much their outputs deviate from expected results as a function of error severity.

We employ a rigorous and controlled experimental setup to systematically introduce varying levels of misspellings into the input, conduct inference with several LLMs, and analyze the degradation in model performance.

## Datasets

### Question Dataset
- A dataset consisting of clean, correctly spelled sentences (questions) derived from the ELI5 (Explain Like I’m Five) corpus.
- These sentences serve as the base for generating corrupted inputs.

### Misspelling Dataset
- A dictionary-based dataset mapping correctly spelled words to their common misspelled variants.
- Each word is associated with one or more realistic misspellings, reflecting typical human spelling errors.

### Corrupted Sentence Generation
- Corrupted sentences were generated by merging the question dataset and the misspelling dataset.
- For each sentence, a predefined number of words (ranging from 1 to 10) were randomly selected and replaced by a corresponding misspelled form from the misspelling dataset.
- This controlled corruption process ensures:
  - The exact number of misspellings per sentence is known and can be varied systematically.
  - The overall semantic meaning of the sentence is preserved.
- This methodology enables precise evaluation of model robustness to syntactic perturbations by introducing realistic and controlled errors.

## Methodology

### Error Injection
- Misspellings are introduced by replacing words in the input sentences using a real-world misspelling dictionary.
- The number of errors per sentence is strictly controlled, ranging from 1 to 10 misspelled words.
- Each corrupted variant maintains the original sentence structure and meaning as much as possible to isolate the impact of syntactic noise.
- This method ensures exact ground-truth knowledge of the number of inserted errors, enabling precise evaluation of robustness.

### Model Inference
- Fifteen (15) state-of-the-art language models are selected, varying widely in size from hundreds of millions to tens of billions of parameters.
- Each model generates outputs for the corrupted input sentences.

### Similarity Computation
- Outputs are embedded into a latent semantic space using a sentence embedding model (`all-mpnet-base-v2`) from the SentenceTransformers library.
- Cosine similarity is computed between the embeddings of the clean and corrupted outputs.
- Cosine similarity values are rescaled from $begin:math:text$[-1, 1]$end:math:text$ to $begin:math:text$[0, 100]$end:math:text$ to enhance interpretability:

$begin:math:display$
\\text{Similarity Score} = (\\text{Cosine Similarity} + 1) \\times 50
$end:math:display$

- Higher similarity scores indicate greater resilience to the input misspellings.

### Analysis
- We analyze how the similarity score degrades as the number of errors increases.
- We calculate the degradation slope for each model to quantify robustness loss.
- Correlation analysis is performed between model size (number of parameters) and robustness.

## Models Evaluated

| Model Name | 
|------------|
| `tiiuae/falcon-7b-instruct` |
| `EleutherAI/gpt-j-6B` |
| `mosaicml/mpt-7b-instruct` |
| `mosaicml/mpt-7b-chat` |
| `mistralai/Mistral-7B-v0.1` |
| `HuggingFaceH4/zephyr-7b-beta` |
| `openchat/openchat-3.5-0106` |
| `Salesforce/codegen-350M-mono` |
| `mistralai/Mixtral-8x7B-Instruct-v0.1` |
| `tiiuae/falcon-40b-instruct` |
| `facebook/opt-1.3b` |
| `facebook/opt-2.7b` |
| `bigscience/bloom-3b` |
| `EleutherAI/gpt-neo-1.3B` |
| `EleutherAI/gpt-neo-2.7B` |

## Key Results

### Correlation Between Model Size and Robustness

We observed a **Pearson correlation of 0.83** between model size and mean similarity score across models — a remarkably high correlation.
This strong positive correlation suggests that larger models are significantly more robust to input noise, maintaining higher semantic fidelity even as the number of misspellings increases.

**Interpretation**:
- Larger models not only perform better on clean data but are also more resilient to syntactic corruption.
- This finding has important implications for deploying language models in noisy, user-generated environments.

## Selected Visualizations

### Similarity Score Degradation with Error Count

![Mean similarity scores vs error count](Readme_Images/similarity_vs_error_count.png)

> *Figure 1: Mean similarity scores for each model as a function of the number of misspellings in the input. A steady decline is observed, with larger models showing slower degradation.*

### Distribution of Similarity Scores by Error Count (Boxplot)

![Boxplot of similarity scores by error count](Readme_Images/similarity_distribution_boxplot.png)

> *Figure 2: Boxplot showing the distribution of similarity scores for different numbers of misspellings aggregated over all models.*

### Distribution of Similarity Scores by Error Count (Violin Plot)

![Violin plot of similarity scores by error count](Readme_Images/similarity_distribution_violinplot.png)

> *Figure 3: Violin plot showing the distribution of similarity scores for different numbers of misspellings aggregated over all models.*

### Mean Similarity Score vs. Error Count per Model (Ordered by Model Size)

![Mean similarity score vs error count per model](Readme_Images/mean_similarity_vs_error_count_per_model.png)

> *Figure 4: Mean similarity score vs. error count for each model, with models ordered by size. Each line shows how robustness changes as error count increases.*

### Distribution of Similarity Scores per Model (Ordered by Model Size)

![Boxplot of similarity scores per model ordered by model size](Readme_Images/similarity_distribution_per_model_ordered.png)

> *Figure 5: Distribution of similarity scores for each model, ordered by model size. Models are displayed from smallest to largest, showing how performance varies with model capacity.*

### Distribution of Similarity Scores Per Model

![Boxplot of similarity scores per model](Readme_Images/similarity_distribution_per_model.png)

> *Figure 6: Distribution of similarity scores for each model, ordered by model size. Boxplots are colored according to model size, and a colorbar indicates the parameter scale.*

### Robustness Degradation Rate

![Barplot of robustness degradation slopes](Readme_Images/robustness_degradation_slope.png)

> *Figure 7: Degradation slope for each model. Higher slopes indicate slower performance decline under increasing input noise.*

### Model Size vs Mean Similarity Score

![Scatter plot of model size vs mean similarity](Readme_Images/model_size_vs_similarity.png)

> *Figure 8: Scatter plot showing model size (log-scaled) versus mean similarity score. The red line represents the best-fit linear regression. A Pearson correlation of 0.83 indicates a strong positive relationship.*

## Word Count Comparison Analysis

To complement the similarity-based robustness evaluation, we analyze how the output length (in terms of word count) changes as a function of the number of misspellings introduced into the input.

For each model and each error level:
- We compute the percentage change in word count between the output for the corrupted input and the output for the clean input.
- The change is expressed relative to the original word count to normalize across sentences of different lengths.

This analysis reveals whether models tend to generate longer or shorter outputs in response to input perturbations and whether model size correlates with stability in output length.

### Word Count Change vs. Error Count

![Line plot of word count change vs. error count](Readme_Images/word_count_change_vs_error_count.png)

> *Figure 9: Average percentage change in output word count as a function of the number of misspellings for each model. A positive change indicates longer outputs, while a negative change indicates shorter outputs. Larger models show greater stability with smaller deviations in output length.*

## Conclusion

This study demonstrates that larger language models are more robust to syntactic perturbations introduced via controlled misspellings. The strong correlation between model size and robustness suggests that parameter scaling not only improves performance on clean data but also enhances stability under noisy conditions.

These results highlight the importance of considering robustness when selecting models for deployment in real-world applications where user input is often noisy or unstructured.

## Notes

- Future work may extend this analysis to other types of input corruption, such as grammatical errors or code-switching.
- Additional experiments could analyze the impact of fine-tuning models specifically for robustness to noisy data.