{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Results Analysis and Visualization\n",
    "\n",
    "This notebook loads the model outputs, computes multiple similarity metrics by comparing each model's baseline output\n",
    "(error_count==0) with outputs for perturbed variants, and produces extensive visualizations.\n",
    "\n",
    "The similarity metrics computed are:\n",
    "   - Difflib similarity\n",
    "   - Cosine similarity based on sentence embeddings\n",
    "   - BLEU score\n",
    "   - **Jaccard similarity** (new metric based on token overlap)\n",
    "\n",
    "Visualizations include per-model boxplots, histograms, density plots, scatter plots (grouped by word count),\n",
    "global line plots of mean/median performance, and additional distribution analyses.\n"
   ],
   "id": "7ad42efaa30c179f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\")\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from src.score_metrics import difflib_similarity, embedding_similarity, bleu_score, jaccard_similarity\n",
    "\n",
    "# Load model outputs\n",
    "df_outputs = pd.read_csv('data/model_outputs.csv')\n",
    "print(\"Loaded model outputs:\", df_outputs.shape)\n",
    "\n",
    "# Add word count (number of words in the variant question)\n",
    "df_outputs['word_count'] = df_outputs['variant_question'].apply(lambda x: len(x.split()))\n",
    "\n",
    "# Initialize similarity score columns for each metric\n",
    "df_outputs['difflib_score'] = np.nan\n",
    "df_outputs['embedding_score'] = np.nan\n",
    "df_outputs['bleu_score'] = np.nan\n",
    "df_outputs['jaccard_score'] = np.nan\n",
    "\n",
    "# For each group (original_question, model_name), use error_count==0 as baseline and compute scores\n",
    "for (orig, model), group in df_outputs.groupby(['original_question', 'model_name']):\n",
    "    baseline_row = group[group['error_count'] == 0]\n",
    "    if baseline_row.empty:\n",
    "        continue\n",
    "    baseline_text = baseline_row.iloc[0]['model_output']\n",
    "    for idx, row in group.iterrows():\n",
    "        variant_text = row['model_output']\n",
    "        df_outputs.at[idx, 'difflib_score'] = difflib_similarity(baseline_text, variant_text)\n",
    "        df_outputs.at[idx, 'embedding_score'] = embedding_similarity(baseline_text, variant_text)\n",
    "        df_outputs.at[idx, 'bleu_score'] = bleu_score(baseline_text, variant_text)\n",
    "        df_outputs.at[idx, 'jaccard_score'] = jaccard_similarity(baseline_text, variant_text)\n",
    "\n",
    "# Save the scored outputs for further evaluation\n",
    "df_outputs.to_csv('data/model_outputs_scored.csv', index=False)\n",
    "print(\"Scored outputs saved at data/model_outputs_scored.csv\")\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Detailed Per-Model Visualizations\n",
    "For each model and for each similarity metric, we generate:\n",
    "   - Boxplots by error count\n",
    "   - Histograms and density plots for each error level\n",
    "   - Scatter plots of similarity versus sentence word count\n"
   ],
   "id": "ae84261eac905221"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "models = df_outputs['model_name'].unique()\n",
    "metrics = ['difflib_score', 'embedding_score', 'bleu_score', 'jaccard_score']\n",
    "\n",
    "for metric in metrics:\n",
    "    for model in models:\n",
    "        model_data = df_outputs[df_outputs['model_name'] == model]\n",
    "\n",
    "        # Boxplot by error count\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.boxplot(x='error_count', y=metric, data=model_data)\n",
    "        plt.title(f\"{model} - {metric} by Error Count\")\n",
    "        plt.xlabel(\"Number of Misspellings\")\n",
    "        plt.ylabel(f\"{metric} (0-100)\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Histogram and density plot for each error_count\n",
    "        error_levels = sorted(model_data['error_count'].unique())\n",
    "        for error in error_levels:\n",
    "            plt.figure(figsize=(8, 5))\n",
    "            subset = model_data[model_data['error_count'] == error][metric].dropna()\n",
    "            plt.hist(subset, bins=30, alpha=0.6, density=True, label=\"Histogram\")\n",
    "            sns.kdeplot(subset, label=\"Density\", color=\"orange\")\n",
    "            plt.title(f\"{model} - {metric} (Error Count = {error})\")\n",
    "            plt.xlabel(f\"{metric} (0-100)\")\n",
    "            plt.ylabel(\"Density\")\n",
    "            plt.legend()\n",
    "            plt.tight_layout()\n",
    "            plt.show()"
   ],
   "id": "aace49e3d1220110"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Global Comparative Visualizations\n",
    "Here we aggregate similarity scores across models and plot global comparisons including:\n",
    "   - Line plots of mean and median metrics across error counts\n",
    "   - Scatter plots of similarity versus word count (grouped by error count)\n",
    "   - Global distribution histograms and density plots for each metric\n"
   ],
   "id": "c3f31c59b1deac9b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "agg_data = df_outputs.groupby(['model_name', 'error_count']).agg({\n",
    "    'difflib_score': ['mean', 'median'],\n",
    "    'embedding_score': ['mean', 'median'],\n",
    "    'bleu_score': ['mean', 'median'],\n",
    "    'jaccard_score': ['mean', 'median']\n",
    "}).reset_index()\n",
    "agg_data.columns = ['model_name', 'error_count',\n",
    "                    'difflib_mean', 'difflib_median',\n",
    "                    'embedding_mean', 'embedding_median',\n",
    "                    'bleu_mean', 'bleu_median',\n",
    "                    'jaccard_mean', 'jaccard_median']\n",
    "print(agg_data.head())\n",
    "\n",
    "for metric in ['difflib_mean', 'embedding_mean', 'bleu_mean', 'jaccard_mean']:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for model in models:\n",
    "        subset = agg_data[agg_data['model_name'] == model]\n",
    "        plt.plot(subset['error_count'], subset[metric], marker='o', label=model)\n",
    "    plt.xlabel(\"Number of Misspellings\")\n",
    "    plt.ylabel(f\"Average {metric.split('_')[0].title()} Score\")\n",
    "    plt.title(f\"Global Comparison: Average {metric.split('_')[0].title()} Score by Error Count\")\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize='small')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Scatter plots: Similarity vs. Word Count (grouped by error_count)\n",
    "for metric in metrics:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.scatterplot(x='word_count', y=metric, hue='error_count', data=df_outputs, palette=\"viridis\", alpha=0.7)\n",
    "    plt.title(f\"{metric} vs. Sentence Word Count (Grouped by Error Count)\")\n",
    "    plt.xlabel(\"Word Count\")\n",
    "    plt.ylabel(f\"{metric} (0-100)\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Additional global distributions for each metric\n",
    "for metric in metrics:\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    subset = df_outputs[metric].dropna()\n",
    "    plt.hist(subset, bins=40, alpha=0.5, density=True, label=\"Histogram\")\n",
    "    sns.kdeplot(subset, label=\"Density\", color=\"red\")\n",
    "    plt.title(f\"Global Distribution of {metric}\")\n",
    "    plt.xlabel(f\"{metric} (0-100)\")\n",
    "    plt.ylabel(\"Density\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "id": "52af129d3b9f5d0a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
