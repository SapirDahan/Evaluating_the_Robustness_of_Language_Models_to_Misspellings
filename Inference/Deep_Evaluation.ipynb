{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Deep Evaluation and Statistical Analysis\n",
    "In this notebook we generate comprehensive evaluation tables and carry out additional statistical analyses.\n",
    "We include all four similarity metrics (difflib, embedding cosine, BLEU, and Jaccard) and report:\n",
    "   - Mean, standard deviation, median, 25th and 75th percentiles per model\n",
    "   - Aggregated tables by model class (e.g., Small, Medium, Large, Standard)\n",
    "   - Additional analyses such as correlation heatmaps, pair plots, and regression analyses.\n",
    "\n",
    "These results help reveal which types of models (by size or architecture) are more robust to misspellings.\n"
   ],
   "id": "e68addaff16f1592"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\")\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import statsmodels.api as sm\n",
    "import scipy.stats as stats\n",
    "import re\n",
    "\n",
    "# Load scored outputs\n",
    "df = pd.read_csv('data/model_outputs_scored.csv')\n",
    "print(\"Loaded scored outputs:\", df.shape)\n",
    "\n",
    "# Add word count if not present\n",
    "if 'word_count' not in df.columns:\n",
    "    df['word_count'] = df['variant_question'].apply(lambda x: len(x.split()))\n",
    "\n",
    "# Classify models by type based on their name (this classification can be extended)\n",
    "def classify_model(model_name):\n",
    "    name = model_name.lower()\n",
    "    if \"large\" in name:\n",
    "        return \"Large\"\n",
    "    elif \"medium\" in name:\n",
    "        return \"Medium\"\n",
    "    elif \"small\" in name:\n",
    "        return \"Small\"\n",
    "    else:\n",
    "        return \"Standard\"\n",
    "\n",
    "df['model_class'] = df['model_name'].apply(classify_model)\n",
    "\n",
    "# Define numeric columns including the new metric (jaccard_score)\n",
    "numeric_cols = ['error_count', 'word_count', 'difflib_score', 'embedding_score', 'bleu_score', 'jaccard_score']\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Comprehensive Evaluation Tables\n",
    "Compute overall descriptive statistics per model and per model class for all similarity metrics.\n",
    "The following tables report mean, standard deviation, median, 25th and 75th percentiles."
   ],
   "id": "ef087f41d24904f7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Evaluation table by model\n",
    "summary_by_model = df.groupby('model_name')[['difflib_score', 'embedding_score', 'bleu_score', 'jaccard_score']].agg(\n",
    "    ['mean', 'std', 'median', lambda x: np.percentile(x, 25), lambda x: np.percentile(x, 75)]\n",
    ").reset_index()\n",
    "summary_by_model.columns = ['model_name',\n",
    "                              'difflib_mean', 'difflib_std', 'difflib_median', 'difflib_q1', 'difflib_q3',\n",
    "                              'embedding_mean', 'embedding_std', 'embedding_median', 'embedding_q1', 'embedding_q3',\n",
    "                              'bleu_mean', 'bleu_std', 'bleu_median', 'bleu_q1', 'bleu_q3',\n",
    "                              'jaccard_mean', 'jaccard_std', 'jaccard_median', 'jaccard_q1', 'jaccard_q3']\n",
    "print(\"Evaluation Table by Model:\")\n",
    "print(summary_by_model.head())\n",
    "summary_by_model.to_csv('data/evaluation_by_model.csv', index=False)\n",
    "print(\"Saved evaluation table by model at data/evaluation_by_model.csv\")\n",
    "\n",
    "# %%\n",
    "# Evaluation table by model class\n",
    "summary_by_class = df.groupby('model_class')[['difflib_score', 'embedding_score', 'bleu_score', 'jaccard_score']].agg(\n",
    "    ['mean', 'std', 'median', lambda x: np.percentile(x, 25), lambda x: np.percentile(x, 75)]\n",
    ").reset_index()\n",
    "summary_by_class.columns = ['model_class',\n",
    "                              'difflib_mean', 'difflib_std', 'difflib_median', 'difflib_q1', 'difflib_q3',\n",
    "                              'embedding_mean', 'embedding_std', 'embedding_median', 'embedding_q1', 'embedding_q3',\n",
    "                              'bleu_mean', 'bleu_std', 'bleu_median', 'bleu_q1', 'bleu_q3',\n",
    "                              'jaccard_mean', 'jaccard_std', 'jaccard_median', 'jaccard_q1', 'jaccard_q3']\n",
    "print(\"Evaluation Table by Model Class:\")\n",
    "print(summary_by_class.head())\n",
    "summary_by_class.to_csv('data/evaluation_by_model_class.csv', index=False)\n",
    "print(\"Saved evaluation table by model class at data/evaluation_by_model_class.csv\")\n"
   ],
   "id": "d7dadf4a767adf44"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Additional Statistical Analyses\n",
    "\n",
    "Generate a correlation heatmap, pair plot, and perform a regression analysis for a selected model.\n"
   ],
   "id": "bacbdb6fc065d22d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Correlation heatmap for the numeric evaluation metrics\n",
    "corr_matrix = df[numeric_cols].corr()\n",
    "plt.figure(figsize=(9, 7))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title(\"Correlation Heatmap: Evaluation Metrics\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# %%\n",
    "# Pair plot for error_count, word_count, and similarity metrics\n",
    "sns.pairplot(df[numeric_cols])\n",
    "plt.suptitle(\"Pair Plot of Evaluation Metrics\", y=1.02)\n",
    "plt.show()\n",
    "\n",
    "# %%\n",
    "# Example regression analysis: effect of error_count on difflib_score for a selected model\n",
    "selected_model = \"gpt2\"\n",
    "df_selected = df[df['model_name'] == selected_model].dropna(subset=['difflib_score'])\n",
    "X = df_selected['error_count']\n",
    "y = df_selected['difflib_score']\n",
    "X = sm.add_constant(X)\n",
    "reg_model = sm.OLS(y, X).fit()\n",
    "print(reg_model.summary())\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.regplot(x='error_count', y='difflib_score', data=df_selected, ci=95, scatter_kws={'alpha': 0.5})\n",
    "plt.title(f\"Regression Analysis: Difflib Similarity vs. Error Count for {selected_model}\")\n",
    "plt.xlabel(\"Error Count\")\n",
    "plt.ylabel(\"Difflib Similarity (0-100)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "4916b8af66a4f3a0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Pairwise Model Comparisons\n",
    "\n",
    "(Optional) Conduct pairwise t-tests between model classes or individual models across error levels to verify significance.\n",
    "(This section can be expanded with further statistical tests as needed.)"
   ],
   "id": "5c0e65de3827ae2"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
